{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import BertForMaskedLM,pipeline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-08T10:00:13.128436Z","iopub.execute_input":"2023-12-08T10:00:13.128744Z","iopub.status.idle":"2023-12-08T10:00:33.485828Z","shell.execute_reply.started":"2023-12-08T10:00:13.128712Z","shell.execute_reply":"2023-12-08T10:00:33.484440Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"bert_model = BertForMaskedLM.from_pretrained('bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2023-12-08T10:00:38.716969Z","iopub.execute_input":"2023-12-08T10:00:38.717807Z","iopub.status.idle":"2023-12-08T10:00:49.145703Z","shell.execute_reply.started":"2023-12-08T10:00:38.717769Z","shell.execute_reply":"2023-12-08T10:00:49.144169Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3709ee13bec943a0ac6b73f43731ff2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96c4e4257e50471f8a03a864ef1b5563"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"print(bert_model)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T10:00:49.173317Z","iopub.execute_input":"2023-12-08T10:00:49.173772Z","iopub.status.idle":"2023-12-08T10:00:49.181006Z","shell.execute_reply.started":"2023-12-08T10:00:49.173733Z","shell.execute_reply":"2023-12-08T10:00:49.180022Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"BertForMaskedLM(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (cls): BertOnlyMLMHead(\n    (predictions): BertLMPredictionHead(\n      (transform): BertPredictionHeadTransform(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (transform_act_fn): GELUActivation()\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Postinal embedding has 512 as max sequence length of bert base is 512 \nToken_type has 2 as there can upto 2 sentences","metadata":{}},{"cell_type":"markdown","source":"**While bert uncased has 30522 tokens , Bert Cased has 28996 tokens. Here in MLM we mask about 15% of corpus for bert to predict. \nIn this after feedforward net and Softmax we get tokens with highest probabiltiy to fit in the blank******","metadata":{}},{"cell_type":"markdown","source":"# Example","metadata":{}},{"cell_type":"code","source":"ex = pipeline(\"fill-mask\",model='bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2023-12-08T10:11:43.124235Z","iopub.execute_input":"2023-12-08T10:11:43.124689Z","iopub.status.idle":"2023-12-08T10:11:43.208779Z","shell.execute_reply.started":"2023-12-08T10:11:43.124658Z","shell.execute_reply":"2023-12-08T10:11:43.207277Z"},"trusted":true},"execution_count":21,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ex \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfill-mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbert_model\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py:950\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    947\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m         \u001b[38;5;66;03m# Impossible to guess what is the right tokenizer here\u001b[39;00m\n\u001b[0;32m--> 950\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    951\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImpossible to guess which tokenizer to use. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m         )\n\u001b[1;32m    955\u001b[0m \u001b[38;5;66;03m# Instantiate tokenizer if needed\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokenizer, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n","\u001b[0;31mException\u001b[0m: Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer."],"ename":"Exception","evalue":"Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.","output_type":"error"}]},{"cell_type":"code","source":"pred = ex(f'Mumbai is a great {ex.tokenizer.mask_token} to live  in.')","metadata":{"execution":{"iopub.status.busy":"2023-12-08T10:10:13.515827Z","iopub.execute_input":"2023-12-08T10:10:13.516241Z","iopub.status.idle":"2023-12-08T10:10:13.643912Z","shell.execute_reply.started":"2023-12-08T10:10:13.516214Z","shell.execute_reply":"2023-12-08T10:10:13.643109Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"for p in pred:\n    print(f\"Token:{p['token_str']}. Score:{100*p['score']}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-08T10:10:13.791382Z","iopub.execute_input":"2023-12-08T10:10:13.792871Z","iopub.status.idle":"2023-12-08T10:10:13.799939Z","shell.execute_reply.started":"2023-12-08T10:10:13.792800Z","shell.execute_reply":"2023-12-08T10:10:13.798258Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Token:city. Score:57.43005871772766\nToken:place. Score:33.80904495716095\nToken:town. Score:0.9498416446149349\nToken:environment. Score:0.7329258602112532\nToken:country. Score:0.7319500669836998\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}